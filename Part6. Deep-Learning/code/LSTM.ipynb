{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import get_tokenizer\n",
    "import torchtext\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "\n",
    "random_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A Beginner’s Guide to Word Embedding with Gensim Word2Vec\\xa0Model',\n",
       "       'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric',\n",
       "       'How to Use ggplot2 in\\xa0Python', ...,\n",
       "       'Content and Marketing Beyond Mass Consumption',\n",
       "       '5 Questions All Copywriters Should Ask Clients Before Their Pen Hits the\\xa0Paper',\n",
       "       'How To Write a Good Business Blog\\xa0Post'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv = pd.read_csv(\"../data/Medium/medium_data.csv\")\n",
    "data = data_csv['title'].values # title 만 사용\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word prediction을 하기 위한 형태로 변환\n",
    "        self.seq = self.pre_zeropadding(seq, self.max_len) # zero padding으로 채워줌\n",
    "        self.X = torch.tensor(self.seq[:,:-1])\n",
    "        self.label = torch.tensor(self.seq[:,-1])\n",
    "\n",
    "    def make_sequence(self, data, vocab, tokenizer):\n",
    "        seq = []\n",
    "        for i in data:\n",
    "            token_id = vocab.lookup_indices(tokenizer(i))\n",
    "            for j in range(1, len(token_id)):\n",
    "                sequence = token_id[:j+1]\n",
    "                seq.append(sequence)\n",
    "        return seq\n",
    "\n",
    "    def pre_zeropadding(self, seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n",
    "        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n",
    "\n",
    "\n",
    "    def __len__(self): # dataset의 전체 길이 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx): # dataset 접근\n",
    "        X = self.X[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(cleaning_text, data))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n",
    "vocab.insert_token('<pad>',0)\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 개수:  5206\n",
      "Validation 개수:  651\n",
      "Test 개수:  651\n"
     ]
    }
   ],
   "source": [
    "# train set과 validation set, test set을 각각 나눕니다. 8 : 1 : 1 의 비율로 나눕니다.\n",
    "train, test = train_test_split(data, test_size = .2, random_state = 42)\n",
    "val, test = train_test_split(test, test_size = .5, random_state = 42)\n",
    "\n",
    "print(\"Train 개수: \", len(train))\n",
    "print(\"Validation 개수: \", len(val))\n",
    "print(\"Test 개수: \", len(test))\n",
    "\n",
    "train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n",
    "valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(test, vocab, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training_loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for texts, labels in tbar:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 순전파\n",
    "\n",
    "        outputs = model(texts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, valid_dataset, criterion, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for texts, labels in tbar:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # _, true_labels = torch.max(labels, dim=1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    valid_accuracy = valid_accuracy / len(valid_dataset)\n",
    "\n",
    "    return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"../models/model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "           x: [batch_size, seq_len]\n",
    "        OUTPUT:\n",
    "           output: [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        # lstm에선 마지막 time step에서의 cell state (c_n)도 반환\n",
    "        output, (h_0, c_0) = self.lstm(x) # output : [batch_size, seq_len, hidden_dim] # h_n: [1, batch_size, hidden_dim] # c_n: [1, batch_size, hidden_dim]\n",
    "        return self.fc(output[:,-1,:]) # [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185cda9fbc2440bad830493235ba5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bf34166bdc471d9f41cc8a9db6d031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 6.9666, Train Accuracy: 0.1077 Valid Loss: 6.7022, Valid Accuracy: 0.1332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e387faf349a040688cfaab887fae889c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696c76bfded64ceaaccb69162aa20409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 5.4563, Train Accuracy: 0.1730 Valid Loss: 6.7635, Valid Accuracy: 0.1456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1259f9be88aa463fbc406583d66de851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c56c0b0518481d8645a8d0d687f5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 4.2166, Train Accuracy: 0.2565 Valid Loss: 7.0157, Valid Accuracy: 0.1454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe1f6fb01704dfcbb48bd63065917ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b9f4943fc04d30be635265bb30f2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 3.0867, Train Accuracy: 0.4072 Valid Loss: 7.3514, Valid Accuracy: 0.1380\n",
      "Early stopping\n",
      "Valid max accuracy :  0.14563106796116504\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'LSTM'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512\n",
    "hidden_size = 256\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d878c51bded4225afd9b195a013b213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word prediction LSTM model accuracy :  0.12664041994750655\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../models/model_LSTM.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_dataloader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_lstm_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction LSTM model accuracy : \", nwp_lstm_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
