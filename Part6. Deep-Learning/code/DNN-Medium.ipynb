{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb1e9fb",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d3df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c6873",
   "metadata": {},
   "source": [
    "#### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5850f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A Beginner’s Guide to Word Embedding with Gensim Word2Vec\\xa0Model',\n",
       "       'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric',\n",
       "       'How to Use ggplot2 in\\xa0Python', ...,\n",
       "       'Content and Marketing Beyond Mass Consumption',\n",
       "       '5 Questions All Copywriters Should Ask Clients Before Their Pen Hits the\\xa0Paper',\n",
       "       'How To Write a Good Business Blog\\xa0Post'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Medium/medium_data.csv\")\n",
    "df = df['title'].values # title 만 사용\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94065f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing\n",
      "['A Beginner’s Guide to Word Embedding with Gensim Word2Vec\\xa0Model'\n",
      " 'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'\n",
      " 'How to Use ggplot2 in\\xa0Python'\n",
      " 'Databricks: How to Save Files in CSV on Your Local\\xa0Computer'\n",
      " 'A Step-by-Step Implementation of Gradient Descent and Backpropagation']\n",
      "After preprocessing\n",
      "['A Beginners Guide to Word Embedding with Gensim Word2Vec Model', 'Handson Graph Neural Networks with PyTorch  PyTorch Geometric', 'How to Use ggplot2 in Python', 'Databricks How to Save Files in CSV on Your Local Computer', 'A StepbyStep Implementation of Gradient Descent and Backpropagation']\n"
     ]
    }
   ],
   "source": [
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_data = list(map(cleaning_text, df)) # 모든 특수문자와 공백을 지움\n",
    "print('Before preprocessing')\n",
    "print(df[:5])\n",
    "print('After preprocessing')\n",
    "print(cleaned_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0dfb0",
   "metadata": {},
   "source": [
    "#### build_vocab_from_iterator\n",
    "\n",
    "- `torchtext.vocab.build_vocab_from_iterator`는 iterator를 이용하여 Vocab 클래스(단어사전)를 만드는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d441a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: A Beginners Guide to Word Embedding with Gensim Word2Vec Model\n",
      "Token: ['a', 'beginners', 'guide', 'to', 'word', 'embedding', 'with', 'gensim', 'word2vec', 'model']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(cleaned_data[0])\n",
    "print(f\"Original text: {cleaned_data[0]}\") # 원본\n",
    "print(f\"Token: {tokens}\") # tokenizer 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d878a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, cleaned_data)) # 단어 사전 생성\n",
    "vocab.insert_token('<pad>', 0) # 패딩 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6185f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', 'to', 'the', 'a', 'of', 'and', 'how', 'in', 'your', 'for']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index > 단어\n",
    "id2token = vocab.get_itos()\n",
    "id2token[:10] # 각 인덱스의 단어 출력, list 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f31ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 0\n",
      "to 1\n",
      "the 2\n",
      "a 3\n",
      "of 4\n",
      "and 5\n"
     ]
    }
   ],
   "source": [
    "# 단어 > index\n",
    "token2id = vocab.get_stoi()\n",
    "token2id = dict(sorted(token2id.items(), key=lambda item: item[1]))\n",
    "for idx, (k, v) in enumerate(token2id.items()):\n",
    "    print(k, v)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c4e72",
   "metadata": {},
   "source": [
    "- `lookup_indices()`: 리스트 형태의 토큰을 받아서 해당 토큰의 인덱스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34699699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 273, 66, 1, 467, 1582, 12, 2884, 8549, 99]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 token 화하고 각 token 을 index로 변환\n",
    "vocab.lookup_indices(tokenizer(cleaned_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf00e15",
   "metadata": {},
   "source": [
    "- Padding: NLP에서는 문장의 길이를 맞추기 위해 짧은 문장 끝에 0을 채워 넣는 작업. 주로 미니배치 학습을 위해 사용됨\n",
    "- ex\n",
    "    ```\n",
    "    문장1: 나는 밥을 먹었다      → 토큰 수: 5\n",
    "    문장2: 밥 먹자               → 토큰 수: 2\n",
    "    문장3: 오늘 날씨 어때?       → 토큰 수: 4\n",
    "    ```\n",
    "    ERROR 발생\n",
    "\n",
    "    ```\n",
    "    [나는, 밥을, 먹었다]       → [나는, 밥을, 먹었다, 0, 0]\n",
    "    [밥, 먹자]                → [밥, 먹자, 0, 0, 0]\n",
    "    ```\n",
    "    해결: Padding 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcd118f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 273],\n",
       " [3, 273, 66],\n",
       " [3, 273, 66, 1],\n",
       " [3, 273, 66, 1, 467],\n",
       " [3, 273, 66, 1, 467, 1582]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = []\n",
    "for i in cleaned_data:\n",
    "    token_id = vocab.lookup_indices(tokenizer(i)) # 문장을 토큰화 → 인덱스로 변환\n",
    "    for j in range(1, len(token_id)): # 2개 단어부터 슬리이싱\n",
    "        sequence = token_id[:j+1]\n",
    "        seq.append(sequence)\n",
    "\n",
    "seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a170e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(sublist) for sublist in seq)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4184432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 273])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_zeropadding(seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n",
    "    return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n",
    "\n",
    "zero_padding_data = pre_zeropadding(seq, max_len)\n",
    "zero_padding_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "303c78f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   3 273]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   3 273  66]] \n",
      "\n",
      "[273  66   1]\n"
     ]
    }
   ],
   "source": [
    "input_x = zero_padding_data[:,:-1]\n",
    "label = zero_padding_data[:,-1]\n",
    "\n",
    "print(input_x[:3], \"\\n\")\n",
    "print(label[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072791c",
   "metadata": {},
   "source": [
    "#### Custom Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f5818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        seq = self.make_sequence(self.data, self.vocab, self.tokenizer)\n",
    "        self.seq = self.pre_zeropadding(seq, self.max_len) # zero padding 처리\n",
    "        self.X = torch.tensor(self.seq[:,:-1])\n",
    "        self.label = torch.tensor(self.seq[:,-1])\n",
    "\n",
    "    # sequence 생성\n",
    "    def make_sequence(self, data, vocab, tokenizer):\n",
    "        seq = []\n",
    "        for i in data:\n",
    "            token_id = vocab.lookup_indices(tokenizer(i))\n",
    "            for j in range(1, len(token_id)):\n",
    "                sequence = token_id[:j+1]\n",
    "                seq.append(sequence)\n",
    "        return seq\n",
    "    \n",
    "    # padding\n",
    "    def pre_zeropadding(self, seq, max_len):\n",
    "        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        label = self.label[idx]\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b090d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text\n",
    "\n",
    "data = list(map(cleaning_text, df))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n",
    "vocab.insert_token('<pad>',0)\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a9e01e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5206, val: 651, test: 651\n"
     ]
    }
   ],
   "source": [
    "# 분리\n",
    "train, test = train_test_split(data, test_size=.2, random_state=42)\n",
    "val, test = train_test_split(test, test_size=.5, random_state=42)\n",
    "\n",
    "print(f\"train: {len(train)}, val: {len(val)}, test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d66e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n",
    "val_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(test, vocab, tokenizer, max_len)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0acefb",
   "metadata": {},
   "source": [
    "#### Next word prediction Model\n",
    "\n",
    "- DNN Model 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "734dad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Next_Word_Prediction_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_dims, num_classes, dropout_ratio, set_super):\n",
    "        if set_super:\n",
    "            super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dims, padding_idx = 0) # padding index 설정 => gradient 계산에서 제외\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_classes = num_classes\n",
    "        for i in range(len(self.hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))\n",
    "            self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1])) # Normalization\n",
    "            self.layers.append(nn.ReLU()) # Activation Function\n",
    "            self.layers.append(nn.Dropout(dropout_ratio))\n",
    "\n",
    "        self.classifier = nn.Linear(self.hidden_dims[-1], self.num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            x: [batch_size, sequence_len] # padding 제외\n",
    "        OUTPUT:\n",
    "            output : [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        x = torch.sum(x, dim=1) # [batch_size, embedding_dim] 각 문장에 대해 임베딩된 단어들을 합쳐서, 해당 문장에 대한 임베딩 벡터로 만듬\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        output = self.classifier(x) # [batch_size, num_classes]\n",
    "        output = self.softmax(output) # [batch_size, num_classes]\n",
    "        return output\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed659144",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2359b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 코드, evaluation 코드, training loop 코드\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    tbar = tqdm(dataloader)\n",
    "    for texts, labels in tbar:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(texts)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실과 정확도 계산\n",
    "        train_loss += loss.item()\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 에폭별 학습 결과 출력\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # model의 업데이트 막기\n",
    "        tbar = tqdm(dataloader)\n",
    "        for texts, labels in tbar:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 손실과 정확도 계산\n",
    "            valid_loss += loss.item()\n",
    "            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # _, true_labels = torch.max(labels, dim=1)\n",
    "            valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    valid_loss = valid_loss / len(dataloader)\n",
    "    valid_accuracy = valid_accuracy / len(val_dataset)\n",
    "\n",
    "    return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "            valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e20b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 5.9816: 100%|██████████| 1159/1159 [01:02<00:00, 18.41it/s]\n",
      "Epoch [1/100], Valid Loss: 7.8286: 100%|██████████| 149/149 [00:00<00:00, 156.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 7.3780, Train Accuracy: 0.0637 Valid Loss: 7.2292, Valid Accuracy: 0.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 6.6932: 100%|██████████| 1159/1159 [01:00<00:00, 19.25it/s]\n",
      "Epoch [2/100], Valid Loss: 8.3096: 100%|██████████| 149/149 [00:00<00:00, 164.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 6.7416, Train Accuracy: 0.0751 Valid Loss: 7.2252, Valid Accuracy: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 7.1709: 100%|██████████| 1159/1159 [01:03<00:00, 18.30it/s]\n",
      "Epoch [3/100], Valid Loss: 8.7249: 100%|██████████| 149/149 [00:00<00:00, 159.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 6.3790, Train Accuracy: 0.0851 Valid Loss: 7.2903, Valid Accuracy: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 5.4916: 100%|██████████| 1159/1159 [00:59<00:00, 19.54it/s]\n",
      "Epoch [4/100], Valid Loss: 8.9234: 100%|██████████| 149/149 [00:00<00:00, 162.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 6.0734, Train Accuracy: 0.0938 Valid Loss: 7.4722, Valid Accuracy: 0.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 5.3046: 100%|██████████| 1159/1159 [00:57<00:00, 20.14it/s]\n",
      "Epoch [5/100], Valid Loss: 10.0305: 100%|██████████| 149/149 [00:00<00:00, 151.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 5.8038, Train Accuracy: 0.1038 Valid Loss: 7.6084, Valid Accuracy: 0.0922\n",
      "Early stopping\n",
      "Valid max accuracy :  0.09223300970873786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 1e-3\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "embedding_dims = 512\n",
    "hidden_dims = [embedding_dims, embedding_dims*4, embedding_dims*2, embedding_dims]\n",
    "model = Next_Word_Prediction_Model(vocab_size = vocab_size, embedding_dims = embedding_dims, hidden_dims = hidden_dims, num_classes = vocab_size, \\\n",
    "            dropout_ratio = 0.2, set_super = True).to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'next'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.NLLLoss(ignore_index=0) # padding 한 부분 제외\n",
    "model, valid_max_accuracy = training_loop(model, train_loader, val_loader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13234a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:00<00:00, 239.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word prediction DNN model accuracy :  0.0726159230096238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_next.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_dnn_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction DNN model accuracy : \", nwp_dnn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33defb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8618\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
