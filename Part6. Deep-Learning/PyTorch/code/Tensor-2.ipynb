{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tensor 연산 및 조작\n",
    "\n",
    "- 1-1. Tensor 간의 계산\n",
    "- 1-2. Broadcasting 을 이용한 Tensor 값 변경\n",
    "- 1-3. Broadcasting 을 이용한 차원이 다른 Tensor 간의 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. Tensor 간의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈\n",
      "a+b : \n",
      " tensor([[ 3, -3],\n",
      "        [ 5,  4]])\n",
      "\n",
      "\n",
      "torch.add(a,b) : \n",
      " tensor([[ 3, -3],\n",
      "        [ 5,  4]])\n",
      "------------------------------\n",
      "뺄셈\n",
      "a-b : \n",
      " tensor([[-1,  1],\n",
      "        [-1,  2]])\n",
      "\n",
      "\n",
      "torch.sub(a,b) : \n",
      " tensor([[-1,  1],\n",
      "        [-1,  2]])\n",
      "------------------------------\n",
      "곱셈\n",
      "a*b : \n",
      " tensor([[2, 2],\n",
      "        [6, 3]])\n",
      "\n",
      "\n",
      "torch.mul(a,b) : \n",
      " tensor([[2, 2],\n",
      "        [6, 3]])\n",
      "------------------------------\n",
      "나눗셈\n",
      "a/b : \n",
      " tensor([[0.5000, 0.5000],\n",
      "        [0.6667, 3.0000]])\n",
      "\n",
      "\n",
      "torch.div(a,b) : \n",
      " tensor([[0.5000, 0.5000],\n",
      "        [0.6667, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, -1], [2, 3]])\n",
    "tensor_b = torch.tensor([[2, -2] ,[3, 1]])\n",
    "\n",
    "print('덧셈')\n",
    "print(\"a+b : \\n\", tensor_a + tensor_b)\n",
    "print('\\n')\n",
    "print(\"torch.add(a,b) : \\n\", torch.add(tensor_a, tensor_b))\n",
    "\n",
    "print('---'*10)\n",
    "\n",
    "print('뺄셈')\n",
    "print(\"a-b : \\n\", tensor_a - tensor_b)\n",
    "print('\\n')\n",
    "print(\"torch.sub(a,b) : \\n\", torch.sub(tensor_a, tensor_b))\n",
    "\n",
    "print('---'*10)\n",
    "\n",
    "print('곱셈')\n",
    "print(\"a*b : \\n\", tensor_a * tensor_b)\n",
    "print('\\n')\n",
    "print(\"torch.mul(a,b) : \\n\", torch.mul(tensor_a, tensor_b))\n",
    "\n",
    "print('---'*10)\n",
    "\n",
    "print('나눗셈')\n",
    "print(\"a/b : \\n\", tensor_a / tensor_b)\n",
    "print('\\n')\n",
    "print(\"torch.div(a,b) : \\n\", torch.div(tensor_a, tensor_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sum: Tensor의 원소들의 합을 return 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape: torch.Size([2, 2])\n",
      "\n",
      "dimension 지정 안했을 때 :  tensor(10)\n",
      "dim = 0 일 때 :  tensor([4, 6])\n",
      "dim = 1 일 때 :  tensor([3, 7])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor_a)\n",
    "print(f\"Shape: {tensor_a.size()}\\n\")\n",
    "\n",
    "print(\"dimension 지정 안했을 때 : \", torch.sum(tensor_a))  # 모든 원소의 합을 반환 함\n",
    "print(\"dim = 0 일 때 : \", torch.sum(tensor_a, dim=0))  # 행을 기준 (행 인덱스 변화)으로 합함 (0행 0열 + 1행 0열, 0행 1열 + 1행 1열)\n",
    "print(\"dim = 1 일 때 : \", torch.sum(tensor_a, dim=1)) # 열을 기준 (열 인덱스 변화)으로 합함 (0행 0열 + 0행 1열, 1행 0열 + 1행 1열)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean: Tensor 원소들의 평균을 return 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Shape: torch.Size([2, 2])\n",
      "\n",
      "dimension 지정 안했을 때 :  tensor(2.5000)\n",
      "dim = 0 일 때 :  tensor([2., 3.])\n",
      "dim = 1 일 때 :  tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # float 지정\n",
    "print(tensor_a)\n",
    "print(f\"Shape: {tensor_a.size()}\\n\")\n",
    "\n",
    "print(\"dimension 지정 안했을 때 : \", torch.mean(tensor_a))  # 모든 원소의 평균을 반환\n",
    "print(\"dim = 0 일 때 : \", torch.mean(tensor_a, dim=0))  \n",
    "print(\"dim = 1 일 때 : \", torch.mean(tensor_a, dim=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max: Tensor 원소들의 가장 큰 값을 return\n",
    "- min: Tensor 원소들의 가장 작은 값을 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape: torch.Size([2, 2])\n",
      "\n",
      "dimension 지정 안했을 때: 4\n",
      "dim = 0 일 때: tensor([3, 4])\n",
      "dim = 1 일 때: tensor([2, 4])\n",
      "\n",
      "dimension 지정 안했을 때: 1\n",
      "dim = 0 일 때: tensor([1, 2])\n",
      "dim = 1 일 때: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor_a)\n",
    "print(f\"Shape: {tensor_a.size()}\\n\")\n",
    "\n",
    "print(f\"dimension 지정 안했을 때: {torch.max(tensor_a)}\")  # 모든 원소 중 최댓값 반환\n",
    "print(f\"dim = 0 일 때: {torch.max(tensor_a, dim=0).values}\")  \n",
    "print(f\"dim = 1 일 때: {torch.max(tensor_a, dim=1).values}\\n\") \n",
    "\n",
    "print(f\"dimension 지정 안했을 때: {torch.min(tensor_a)}\")  # 모든 원소의 최솟값 반환 함\n",
    "print(f\"dim = 0 일 때: {torch.min(tensor_a, dim=0).values}\")  \n",
    "print(f\"dim = 1 일 때: {torch.min(tensor_a, dim=1).values}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- argmax: Tensor 원소들의 가장 큰 값의 위치 반환\n",
    "- argmin: Tensor 원소들의 가장 작은 값의 위치 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape: torch.Size([2, 2])\n",
      "\n",
      "dimension 지정 안했을 때 : 3\n",
      "dim = 0 일 때 : tensor([1, 1])\n",
      "dim = 1 일 때 : tensor([1, 1])\n",
      "\n",
      "dimension 지정 안했을 때 : 0\n",
      "dim = 0 일 때 : tensor([0, 0])\n",
      "dim = 1 일 때 : tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor_a)\n",
    "print(f\"Shape: {tensor_a.size()}\\n\")\n",
    "\n",
    "print(f\"dimension 지정 안했을 때 : {torch.argmax(tensor_a)}\")  # 모든 원소 중 최댓값 위치 반환\n",
    "print(f\"dim = 0 일 때 : {torch.argmax(tensor_a, dim=0)}\")  \n",
    "print(f\"dim = 1 일 때 : {torch.argmax(tensor_a, dim=1)}\\n\") \n",
    "\n",
    "print(f\"dimension 지정 안했을 때 : {torch.argmin(tensor_a)}\")  # 모든 원소의 최솟값 위치 반환\n",
    "print(f\"dim = 0 일 때 : {torch.argmin(tensor_a, dim=0)}\")  \n",
    "print(f\"dim = 1 일 때 : {torch.argmin(tensor_a, dim=1)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dot: 벡터의 내적 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1.dot(u1): 11\n",
      "torch.dot(v1, u1): 11\n"
     ]
    }
   ],
   "source": [
    "v1 = torch.tensor([1, 2])\n",
    "u1 = torch.tensor([3, 4])\n",
    "\n",
    "print(f\"V1.dot(u1): {v1.dot(u1)}\") # 1x3 + 2x4 = 11\n",
    "print(f\"torch.dot(v1, u1): {torch.dot(v1, u1)}\") # 1x3 + 2x4 = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matmul: 두 Tensor 간의 행렬곱 반환\n",
    "- 원소 곱셈과 다름 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "B: tensor([[-1,  2],\n",
      "        [ 1,  0]])\n",
      "\n",
      "AB: tensor([[1, 2],\n",
      "        [1, 6]])\n",
      "BA: tensor([[5, 6],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[-1, 2], [1, 0]])\n",
    "\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\\n\")\n",
    "\n",
    "print(f\"AB: {torch.matmul(A, B)}\") # A 에서 B를 행렬곱\n",
    "print(f\"BA: {B.matmul(A)}\") # B 에서 A를 행렬곱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. Broadcasting 을 이용한 Tensor 값 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scalar 값으로 Tensor 원소 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([[0.8637, 0.6219],\n",
      "        [0.9647, 0.0415],\n",
      "        [0.9986, 0.9032]])\n",
      "\n",
      "변경된 Tensor: tensor([[10.0000, 10.0000],\n",
      "        [ 0.9647,  0.0415],\n",
      "        [ 0.9986,  0.9032]])\n"
     ]
    }
   ],
   "source": [
    "torch_a = torch.rand(3, 2)\n",
    "print(f\"Original Tensor: {torch_a}\\n\")\n",
    "\n",
    "torch_a[0, :] = 10 # 0행의 모든 열에 broadcasting 을 통한 scalar 값 대입\n",
    "print(f\"변경된 Tensor: {torch_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- indexing 으로 Tensor 원소 접근 후 scalar 값으로 원소 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([[-1.5824,  0.4951],\n",
      "        [-1.4963,  0.0228],\n",
      "        [ 0.8205,  1.1978]])\n",
      "\n",
      "변경된 Tensor: tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randn(3, 2)\n",
    "print(f\"Original Tensor: {tensor_a}\\n\")\n",
    "\n",
    "tensor_a[:, :] = torch.tensor([0, 1]) # 모든 값에 접근해서 [0,1] 로 변경\n",
    "print(f\"변경된 Tensor: {tensor_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sparse Tensor 조작\n",
    "\n",
    "- 2-1. COO Tensor 이해\n",
    "- 2-2. CSC/CSR Tensor 이해\n",
    "- 2-3. Sparse Tensor 의 필요성\n",
    "- 2-4. Sparse Tensor 조작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. COO Tensor 이해\n",
    "\n",
    "- 구조: 튜플 리스트 형식  \n",
    "- 저장 내용: (행 인덱스, 열 인덱스, 값)  \n",
    "- 특징: 구조 단순, 수정과 생성이 쉬움  \n",
    "- 주로 사용되는 경우: 연산보다는 **데이터 입력/생성 시**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1],\n",
       "                       [1, 0]]),\n",
       "       values=tensor([2, 3]),\n",
       "       size=(2, 2), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0, 2], [3, 0]])\n",
    "a.to_sparse() # COO Sparse Tensor 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Sparse_coo_tensot`: COO 형식의 Sparse Tensor 를 생성하는 함수\n",
    "    - indices: 0이 아닌 값을 가진 행, 열의 위치\n",
    "    - values: 0이 아닌 값\n",
    "    - nnz: 0이 아닌 값의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 1]]),\n",
      "       values=tensor([4, 5, 6]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "\n",
      "tensor([[0, 0, 4],\n",
      "        [5, 6, 0]])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.tensor([[0, 1, 1], [2, 0, 1]])\n",
    "values = torch.tensor([4, 5, 6])\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices=indices, values=values, size=(2, 3))\n",
    "\n",
    "print(f\"{sparse_tensor}\\n\")\n",
    "print(sparse_tensor.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. CSC/CSR Tensor 이해\n",
    "\n",
    "- CSR\n",
    "    - 구조: 압축된 행 기반  \n",
    "    - 저장 내용: 행 포인터, 열 인덱스, 값  \n",
    "    - 특징: **행 단위 연산**이 빠름  \n",
    "    - 주로 사용되는 경우: **행 방향 탐색**에 유리 (ML, 분류 등)\n",
    "\n",
    "- CSC\n",
    "    - 구조: 압축된 열 기반  \n",
    "    - 저장 내용: 열 포인터, 행 인덱스, 값  \n",
    "    - 특징: **열 단위 연산**이 빠름  \n",
    "    - 주로 사용되는 경우: **열 방향 탐색**에 유리 (선형대수, 수치 해석 등)\n",
    "\n",
    "- to_sparse_csr : Dense tensor를 CSR 형식의 Sparse tensor로 변환하는 함수\n",
    "  - crow_indices : 0 이 아닌 값을 가진 행의 위치 (첫번째는 무조건 0)\n",
    "  - col_indices : 0 이 아닌 값을 가진 열의 위치\n",
    "  - values : 0 이 아닌 값\n",
    "  - nnz : 0 이 아닌 값의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 4])\n",
      "\n",
      "tensor([[0, 0, 4, 3],\n",
      "        [5, 6, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([0, 2, 4]),\n",
       "       col_indices=tensor([2, 3, 0, 1]),\n",
       "       values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,\n",
       "       layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n",
    "print(f\"Shape: {t.size()}\\n\")\n",
    "print(t)\n",
    "\n",
    "t.to_sparse_csr() # Dense Tensor 를 CSR 형식의 Sparse Tensor 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 4])\n",
      "\n",
      "tensor([[0, 0, 4, 3],\n",
      "        [5, 6, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),\n",
       "       row_indices=tensor([1, 1, 0, 0]),\n",
       "       values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,\n",
       "       layout=torch.sparse_csc)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n",
    "print(f\"Shape: {t.size()}\\n\")\n",
    "print(t)\n",
    "\n",
    "t.to_sparse_csc() # Dense Tensor 를 CSC 형식의 Sparse Tensor 로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparse_csr_tensor: CSR 형식의 Sparse Tensor 를 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(crow_indices=tensor([0, 2, 2]),\n",
      "       col_indices=tensor([0, 1]),\n",
      "       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "crow_indices = torch.tensor([0, 2, 2]) # 0이 아닌 행의 위치 (첫번쨰는 무조건 0), 즉 row_pointer\n",
    "col_indices = torch.tensor([0, 1]) # 0이 아닌 열의 위치\n",
    "values = torch.tensor([1, 2]) # 0이 아닌 값\n",
    "csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)\n",
    "\n",
    "print(csr)\n",
    "print()\n",
    "print(csr.to_dense()) # CRS Tensor 를 Dense Tensor 로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. Sparse Tensor 의 필요성\n",
    "\n",
    "- 큰 크기의 matrix 를 구성할 때 일반적인 Dense Tensor 는 메모리 아웃 현상이 발생하지만\n",
    "- Sparse Tensor 는 메모리 아웃현상이 발생하지 않음\n",
    "- `to_dense()`: Saprse Tensor 를 Dense Tensor 로 만드는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  542, 64383, 27284,  ..., 34489, 27409, 12997],\n",
       "                       [41868, 18464, 71441,  ..., 56694, 36771, 69393]]),\n",
       "       values=tensor([0.5017, 0.6638, 0.5747,  ..., 0.1194, 0.3796, 0.2016]),\n",
       "       size=(100000, 100000), nnz=100000, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(0, 100000, (200000,)).reshape(2, -1)\n",
    "v = torch.rand(100000)\n",
    "coo_sparse_tensor = torch.sparse_coo_tensor(indices = i, values = v, size = [100000, 100000]) # COO Sparse Tensor (100000 x 100000)\n",
    "coo_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([ 2685,  5730, 64103,  ...,   266, 91733, 44810]),\n",
       "       col_indices=tensor([  160, 17354, 71650,  ..., 62978, 17957, 73504]),\n",
       "       values=tensor([0.6782, 0.1481, 0.6329,  ..., 0.4253, 0.8559, 0.8333]),\n",
       "       size=(99999, 100000), nnz=100000, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crow = torch.randint(0, 100000, (100000,))\n",
    "col = torch.randint(0, 100000, (100000,))\n",
    "v = torch.rand(100000)\n",
    "csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000 x 100000)\n",
    "csr_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COO > Dense Tensor 로 변환 = 메모리 아웃, 즉 커널 튕김\n",
    "coo_sparse_tensor.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSR > Dense Tensor 로 변환 = 메모리 아웃, 즉 커널 튕김\n",
    "csr_sparse_tensor.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4. Sparse Tensor 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "\n",
      "\n",
      "곱셈\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "\n",
      "\n",
      "행렬곱\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Sparse 와 Sparse Tensor 간의 연산 (2차원)\n",
    "a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float)\n",
    "b = torch.tensor([[1, 0],[0, 0]], dtype=torch.float)\n",
    "\n",
    "sparse_a = a.to_sparse()\n",
    "sparse_b = b.to_sparse()\n",
    "\n",
    "print('덧셈')\n",
    "print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())\n",
    "\n",
    "print('\\n')\n",
    "print('곱셈')\n",
    "print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('행렬곱')\n",
    "print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3차원 sparse tensor 에는 일반 텐서와 동일하게 사칙연산 함수들은 사용 가능하지만 행렬곱을 사용할 수 없음\n",
    "- CSR/CSC 형식에서는 곱셈도 3차원에선 불가능\n",
    "- 이는 sparse tensor 와 sparse tensor 간에도 적용이 되고, sparse tensor 와 dense tensor 간의 연산에도 적용이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈\n",
      "tensor([[[True, True],\n",
      "         [True, True]],\n",
      "\n",
      "        [[True, True],\n",
      "         [True, True]]])\n",
      "\n",
      "\n",
      "곱셈\n",
      "tensor([[[True, True],\n",
      "         [True, True]],\n",
      "\n",
      "        [[True, True],\n",
      "         [True, True]]])\n",
      "\n",
      "\n",
      "행렬곱\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:22748 [kernel]\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]\nNamed: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m행렬곱\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmatmul(a, b)\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;241m==\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()) \u001b[38;5;66;03m# 에러 발생\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:22748 [kernel]\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]\nNamed: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:15256 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Sparse 와 Sparse Tensor 간의 연산 (3차원)\n",
    "a = torch.tensor([[[0, 1], [0, 2]], [[0, 1], [0, 2]]], dtype=torch.float)\n",
    "b = torch.tensor([[[1, 0],[0, 0]], [[1, 0], [0, 0]]], dtype=torch.float)\n",
    "\n",
    "sparse_a = a.to_sparse()\n",
    "sparse_b = b.to_sparse()\n",
    "\n",
    "print('덧셈')\n",
    "print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())\n",
    "\n",
    "print('\\n')\n",
    "print('곱셈')\n",
    "print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('행렬곱')\n",
    "print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # 에러 발생"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
